# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Acx1KkuT7VY9MLlTw1fuUyRv_h61kg7
"""

import requests
import xml.etree.ElementTree as ET

# Diyanet işleri başkanlığı ile ilgili Google News RSS URL'si
rss_url = "https://news.google.com/rss/search?q=Diyanet+İşleri+Başkanlığı&hl=tr&gl=TR&ceid=TR:tr"

# RSS feed'i çek
response = requests.get(rss_url)

if response.status_code == 200:
    xml_data = response.content

    # XML'i parse et
    root = ET.fromstring(xml_data)

    # RSS’de haberler <item> etiketi altında olur
    for item in root.findall(".//item"):
        baslik = item.find("title").text
        link = item.find("link").text

        print(f"Başlık: {baslik}")
        print(f"Link: {link}")
        print("---")

else:
    print("RSS feed alınamadı, status code:", response.status_code)

import requests
import xml.etree.ElementTree as ET

rss_url = "https://news.google.com/rss/search?q=Diyanet+İşleri+Başkanlığı&hl=tr&gl=TR&ceid=TR:tr"

response = requests.get(rss_url)

if response.status_code == 200:
    xml_data = response.content
    root = ET.fromstring(xml_data)

    for item in root.findall(".//item"):
        baslik = item.find("title").text
        link = item.find("link").text

        # description etiketi genellikle özet içerir
        description_elem = item.find("description")
        description = description_elem.text if description_elem is not None else "Özet yok"

        print(f"Başlık: {baslik}")
        print(f"Link: {link}")
        print(f"Özet: {description}")
        print("---")

else:
    print("RSS feed alınamadı, status code:", response.status_code)

import feedparser

rss_url = "https://news.google.com/rss/search?q=hutbe&hl=tr&gl=TR&ceid=TR:tr"
feed = feedparser.parse(rss_url)

for entry in feed.entries:
    print(f"Başlık: {entry.title}")
    print(f"Link: {entry.link}")
    print(f"Tarih: {entry.published}")
    print("-" * 50)

pip install feedparser

import feedparser
rss_url= "https://www.haberturk.com/rss"

keyword="rüya"

feed=feedparser.parse(rss_url)

for entry in feed.entries:
  if keyword.lower() in entry.title.lower() or keyword.lower() in entry.summary.lower():
    print(entry.title)
    print(entry.link)
    print(entry.description)
    print("------")

import feedparser
from datetime import datetime, timedelta
import pytz

# RSS URL
rss_url = "https://news.google.com/rss/search?q=hutbe&hl=tr&gl=TR&ceid=TR:tr"

# Bugünün tarihi
today = datetime.now(pytz.timezone('Europe/Istanbul'))

# Son cuma
days_since_friday = (today.weekday() - 4) % 7  # Cuma = 4
last_friday = today - timedelta(days=days_since_friday)

print(f"Bugün: {today.strftime('%d %B %Y, %A')}")
print(f"Son Cuma: {last_friday.strftime('%d %B %Y, %A')}")
print("-" * 50)

# RSS feedi
feed = feedparser.parse(rss_url)

# Filtreleme
for entry in feed.entries:
    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')
    pub_date = pub_date.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/Istanbul'))

    if pub_date > last_friday:
        print(f"Tarih: {pub_date.strftime('%d %B %Y, %H:%M')}")
        print(f"Başlık: {entry.title}")
        print(f"Link: {entry.link}")
        print("-" * 50)

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pytz

# RSS URL
rss_url = "https://news.google.com/rss/search?q=hutbe&hl=tr&gl=TR&ceid=TR:tr"

# Tarih hesaplama
today = datetime.now(pytz.timezone('Europe/Istanbul'))
days_since_friday = (today.weekday() - 4) % 7
last_friday = today - timedelta(days=days_since_friday)

# RSS çek
feed = feedparser.parse(rss_url)

# Haber içeriği çekme fonksiyonu
def get_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"}
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()

        soup = BeautifulSoup(r.text, "html.parser")

        # Haber sitelerine göre farklı CSS seçiciler denenebilir:
        article_text = ""

        # 1. Yaygın haber içerik blokları
        selectors = [
            "article",               # <article> etiketi
            "div.article-content",   # div.article-content
            "div.entry-content",     # div.entry-content
            "div#content",           # div#content
            "div.text",              # div.text
            "div#haberMetni",        # bazı Türk siteleri
            "div.post-content",
            "div.article-text.container-padding"
        ]

        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                article_text = element.get_text(separator="\n", strip=True)
                break

        if not article_text:
            # Hiçbir seçici çalışmazsa tüm <p> etiketlerini birleştir
            paragraphs = soup.find_all("p")
            article_text = "\n".join([p.get_text(strip=True) for p in paragraphs])

        return article_text.strip()
    except Exception as e:
        return f"[HATA] İçerik alınamadı: {e}"

# RSS'ten haberleri süz ve içeriğini al
for entry in feed.entries:
    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')
    pub_date = pub_date.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/Istanbul'))

    if pub_date > last_friday:
        print(f"Başlık: {entry.title}")
        print(f"Tarih: {pub_date}")
        print(f"Link: {entry.link}")

        content = get_article_content(entry.link)
        print("İçerik Örneği:")
        print(content[:500] + "...\n")  # Uzunluğu kısalttık
        print("=" * 80)

import feedparser
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from datetime import datetime, timedelta
import pytz

# RSS URL
rss_url = "https://news.google.com/rss/search?q=hutbe&hl=tr&gl=TR&ceid=TR:tr"

# Tarih hesaplama
today = datetime.now(pytz.timezone('Europe/Istanbul'))
days_since_friday = (today.weekday() - 4) % 7
last_friday = today - timedelta(days=days_since_friday)

def get_article_content(url):
    """Verilen haber linkinden metin içerik döndürür"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/127.0.0.0 Safari/537.36"
        }
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()

        soup = BeautifulSoup(r.text, "html.parser")
        domain = urlparse(url).netloc

        # Domain'e özel seçiciler
        site_selectors = {
            "www.diyanet.gov.tr": [
                "div.article-text.container-padding"
            ],
            "www.haberturk.com": [
                "div.news-content",
                "div.text"
            ],
            "www.hurriyet.com.tr": [
                "div.news-content",
                "div.content-body"
            ]
            # Buraya başka siteler ekleyebilirsin
        }

        # Seçici listesi: önce domain'e özel, sonra genel
        selectors = site_selectors.get(domain, []) + [
            "article",
            "div.article-content",
            "div.entry-content",
            "div#content",
            "div.text",
            "div#haberMetni",
            "div.post-content"
        ]

        # İçerik arama
        for selector in selectors:
            element = soup.select_one(selector)
            if element and element.get_text(strip=True):
                return element.get_text(separator="\n", strip=True)

        # Eğer hiçbir seçici işe yaramadıysa fallback: tüm <p> etiketleri
        paragraphs = soup.find_all("p")
        if paragraphs:
            return "\n".join([p.get_text(strip=True) for p in paragraphs])

        return "[Uyarı] İçerik bulunamadı."

    except Exception as e:
        return f"[HATA] İçerik alınamadı: {e}"

# RSS'ten haberleri çek ve içeriklerini yazdır
feed = feedparser.parse(rss_url)
for entry in feed.entries:
    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')
    pub_date = pub_date.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/Istanbul'))

    if pub_date > last_friday:
        print(f"Başlık: {entry.title}")
        print(f"Tarih: {pub_date}")
        print(f"Link: {entry.link}")

        content = get_article_content(entry.link)
        print("İçerik (ilk 500 karakter):")
        print(content[:500] + "...\n")
        print("=" * 80)

pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import feedparser
from datetime import datetime, timedelta
import pytz

# RSS URL
rss_url = "https://news.google.com/rss/search?q=rüya&hl=tr&gl=TR&ceid=TR:tr"

# Tarih hesaplama
today = datetime.now(pytz.timezone('Europe/Istanbul'))
days_since_friday = (today.weekday() - 4) % 7
last_friday = today - timedelta(days=days_since_friday)

# Selenium ayarları
options = Options()
options.add_argument("--headless")  # Tarayıcıyı görünmez çalıştırır
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(options=options)

def get_article_content_selenium(url):
    try:
        driver.get(url)
        time.sleep(3)  # JS yüklenmesi için bekleme süresi

        # İçerik için birden fazla seçici deneyelim
        selectors = [
            "div.article-text.container-padding",  # Diyanet
            "article",
            "div.article-content",
            "div.entry-content",
            "div#content",
            "div.text",
            "div#haberMetni",
            "div.post-content"
        ]

        for selector in selectors:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                text = "\n".join([e.text for e in elements if e.text.strip()])
                if text.strip():
                    return text

        # Fallback: tüm paragraf etiketlerini al
        paragraphs = driver.find_elements(By.TAG_NAME, "p")
        if paragraphs:
            return "\n".join([p.text for p in paragraphs if p.text.strip()])

        return "[Uyarı] İçerik bulunamadı."
    except Exception as e:
        return f"[HATA] {e}"

# RSS'ten haberleri çek ve içeriklerini yazdır
feed = feedparser.parse(rss_url)
for entry in feed.entries:
    pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')
    pub_date = pub_date.replace(tzinfo=pytz.UTC).astimezone(pytz.timezone('Europe/Istanbul'))

    if pub_date > last_friday:
        print(f"Başlık: {entry.title}")
        print(f"Tarih: {pub_date}")
        print(f"Link: {entry.link}")

        content = get_article_content_selenium(entry.link)
        print("İçerik (ilk 500 karakter):")
        print(content[:500] + "...\n")
        print("=" * 80)

driver.quit()

print("hello")